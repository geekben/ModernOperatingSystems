##1.3 计算机硬件概览
操作系统与其运行的硬件环境是紧密相关的。它延伸了计算机的指令集并且为计算机管理各种资源。要让计算机正常工作，操作系统必须非常了解硬件，至少要对这些硬件暴露给程序员的接口了如指掌。因此，让我们简要的浏览一下当今计算机系统通常使用的硬件设备。有了这些知识，我们就能开始详细了解操作系统和其运行原理。

一个简单的个人计算机可以被抽象为图1-6那样的模型。CPU，内存和IO设备都连接在一条系统总线上，并通过这条总线相互通信。现代操作系统具有更加复杂的结构，有多种总线，我们后面会谈到。暂时来说，这种模型就足够分析问题了。后面的章节中，我们将简要概述上述各个组成部件，并且深入剖析一些操作系统设计者们关系的硬件问题。不用说，这将是个非常简洁的综述。关于计算机硬件和计算机组成的书有很多。两本最著名的是 Tanenbaum and Austin (2012) 和 Patterson and Hennessy (2013)。

###1.3.1 处理器
计算机的大脑是CPU，它从内存中取指令并执行。CPU最简单的工作流程就是，从内存读取第一条指令，解码以确定其类型和操作数，执行它，然后读取，解码，执行后续的指令。这种流程被一直重复，直到程序结束。程序就是这样被运行的。

每种CPU都有其独有的指令集。因此，x86处理器不能执行ARM的程序，反之亦然。因为访问内存读取指令和数据的时间比执行指令的时间长很多，所以所有CPU都拥有一些寄存器，用于存放关键变量和临时结果。因此，指令集都包含了从内存加载数据到寄存器和保存寄存器数据到内存的指令。其他一些指令，将两个操作数组合为一个结果，这两个操作数可以来自内存也可以来自寄存器，或者各取一个；比如就有指令将两个操作数做加法，结果存入寄存器或者内存中。

除了上述用于存放数据的通用寄存器，大多处理器含有一些程序员可见的特殊寄存器。其中之一是**程序计数器**，它包含了下一条要读取的指令的内存地址。处理器读取了一条指令后，程序计数器将被更新，指向下一条指令。

另一个寄存器是**栈指针**，它指向内存中栈的顶端。栈保存了每个已经开始但尚未结束的程序流程的数据帧。这种数据帧包含了入参，局部变量和没有使用寄存器保存的临时变量。

再一个寄存器是**PSW**（程序状态字）。这个寄存器包含了数个条件码bits(*N(negative)Z(zero)V(overflow)C(carry)*)，它们由比较指令来设置，还包含了CPU优先级寄存器，运行模式（用户态或内核态），还有一些其他控制位。用户态程序通常可以读取整个PSW，但是只能写其中一些位。PSW寄存器在系统调用和IO中扮演了重要角色(*x86的EFlags包含了上面的条件码bits*)。

操作系统必须完全感知这些寄存器。对CPU进行分时复用时，操作系统会经常停止正在运行的程序去启动或者重启另外一个程序。每次它停止一个程序的时候，必须把这些寄存器都保存下来，这样，当这个程序再次运行时就可以恢复他们。

为了提高性能，操作系统的设计者早就放弃了对单一指令读取解析执行这一流程。许多现代CPU都有同时并行执行多条指令的能力。比如，某种CPU可能具有分离的读取，解析，执行单元。于是，当它在执行第n条指令时，可能同时在解析第n+1条指令，并同时在读取第n+2条指令。这种组织方式，被称作流水线，图1-7（a）描述了三阶段的流水线结构。实际上流水线通常更长。大多数流水线设计中，一条指令被读取后，它必须被执行。即使之前的指令是条件转移指令。流水线让编译器和操作系统的作者无比头痛，因为他们将不得不直面底层系统的复杂性。

比流水线设计更复杂的是超标量处理器，如图1-7（b）所示。这种设计中，安排了多个执行单元，比如可以同时拥有一个整型运算器，一个浮点型运算器，一个布尔运算器。一次读取2条或者更多的指令，解析后存放在一个缓冲区中，等待执行。一旦有执行单位空闲出来，立即检查缓冲区，看看有没有自己可以执行的指令在其中，如果有则将这条指令从缓冲区中移除并执行。这种设计带来的一个影响是，指令常常会乱序执行。大部分情况下，由硬件来保证程序乱序执行和顺序执行的结果保持一致，但是还是给操作系统强加了一些恼人的复杂性，后面我们会谈到。

除了一些在嵌入式系统中使用的简单CPU，大部分的CPU都有用户态和内核态两种运行模式。一般由PSW寄存器中的一个bit来控制当前运行模式。当处于内核态时，CPU可以执行指令集里的所有指令，并使用所有硬件特性。在桌面电脑或者服务器上，操作系统一般都运行在内核态，可以访问整个硬件。在大多数嵌入式系统中，只有一小段代码运行在内核态，剩余的操作系统代码都运行在用户态。（*这一句是否说反了，嵌入式系统的用户态程序很少，剩下都是操作系统才对吧*）

用户程序通常运行在用户态，只被允许执行指令集的一个子集，只能访问部分硬件特性。大体上说，牵涉到IO和内存保护的指令都不能在用户态执行。比如，设置PSW寄存器中的运行模式位，让CPU进入内核态，理所当然是被禁止了的。

用户程序必须通过系统调用来获取操作系统提供的服务。系统调用会使程序陷入内核并触发操作系统工作。TRAP指令将系统从用户态切换到内核态并启动操作系统。当操作系统完成请求的任务，它就将控制权交换给用户程序，从刚刚完成的系统调用的下一条指令开始执行。我们将在后面讨论系统调用机制的细节。我们暂时就把系统调用当作一种特殊的程序调用过程，只不过具有从内核态到用户态切换的附加功能而已。
在此说明一下，我们后面会用小写的斜体字代表系统调用名称：*read*。

除了系统调用，其他的指令引起的陷入没有太大的价值。大部分的其他陷入都是硬件对异常情况的告警。比如除零异常和浮点数向下溢出异常。所有的这些情况，操作系统都会得到控制权，从而决定该如何应对。有时，程序必须被终止并返回错误码，有时候则完全可以忽略这些异常（向下溢出的数值可以直接设置为0）。最后，如果程序事先声明了它自己需要对异常情况做一些处理，那么操作系统会把控制权交还给程序，让他们自己有机会处理这些问题。

####多线程和多核芯片
摩尔定律声称，芯片上的集成电路密度每隔18个月翻一番。这个定律并不是某种类似动量守恒那样的物理定律，只不过是摩尔根据自己对半导体公司中那些工艺流程工程师们将二极管体积缩小的速度的观察得出的规律总结。摩尔定律在过去30年中都一直被坚守，而且预计还将在未来十年或者更长时间保持有效性。在这之后，每个二极管上的原子个数将少到足以令量子力学开始产生巨大影响，让继续缩小二极管体积成为不可能。

二极管数量的极大丰富带来了一个问题：我们怎么发挥所有二极管的能力？我们看到了一种方式：具有多种功能单元的超标量架构。但是，二极管数量仍然在增加，可用的越来越多。一件显而易见的事情就是增加CPU上的缓存数，而大家确实就是这么做的，但最终收益拐点还是会来临。

容易想到的下一步就是不仅仅复制多个功能单元，同时也在一个处理器核上复制多份控制逻辑。Intel奔腾4处理器引入了这项技术，被称作多线程或者超线程技术（Intel独有的名称）。不仅在x86处理器上，其他CPU芯片也具备这种新技术，包括SPARC，Power5，英特尔至强，英特尔Core系列。简单来说，这项技术所做的就是在一个CPU上同时保持两个线程上下文，在他们之间不断的进行纳秒级的切换。（线程就是轻量级的进程，而进程就是一个在执行中的程序，这个我们会在第二章里面介绍）举个例子，如果一个进程需要从内存里面读取一个数据（这会消耗很多个CPU周期），此时多线程的CPU可以切换到另一个线程去执行。多线程处理器并没有提供真正的并行性，某一时刻只有一个线程真正在运行，但是线程切换的时间却被缩小到了纳秒级。

多线程技术对操作系统也产生了影响，因为每个线程对操作系统来说就是个独立的CPU。想象一个系统拥有两个CPU，而每一个又有两个超线程，那么将认为系统拥有4个CPU。如果系统负载仅能保持两个CPU处于工作状态，那么操作系统很可能不经意的将这两个线程调度到同一个CPU的两个超线程上，而另一片CPU将完全空闲。这种调度选择显然没有只在CPU使用其中一个线程效率高。

除了超线程，许多CPU芯片都有4个8个甚至更多的完整的核。如图1-8所示的多核处理器实际上包含了4个迷你芯片，每片都是独立的CPU。（缓存将在下面解释）有些处理器，比如英特尔Xeon Phi和Tilera的TilePro，已经将60多个核心放在一个CPU芯片上。使用这样的多核处理器，必须要有个支持多处理器的操作系统。

顺提一句，要比绝对数量，谁也不是现今GPU（graphics processing unit）的对手。GPU是一个可以具有几千个微核心的处理器。他们非常擅长对大量小型计算任务的并行处理，比如在图像处理应用中对边形的渲染。但是他们对串行任务则显得力不从心。同时，编写GPU程序也是很困难的。虽然GPU对操作系统很有用（处理加密或处理网络通信任务），但是操作系统自身却几乎不可能跑在GPU上。

###1.3.2 内存
计算机的第二大组成部分是内存。理想的情况是内存的读取速度要极其快（比执行一条命令还要快，这样CPU就不会因为内存读取而出现停滞状态），无比的大，而且价格贱如泥土。现存的技术无法同时满足上述条件，所以我们必须另辟蹊径。内存系统是由多个层次构成的层级系统，如图1-9所示。上层比下层的速度更快，但是容量更小，单个bit的价格也更昂贵，他们的差别通常都是十亿倍甚至更多。

最上层是由CPU中的寄存器构成。他们和CPU本身的材质相同，所以和CPU一样快。访问他们并没有延时。他们的可用容量通常是32位CPU为32×32bits，而在64位上就是64×64bits。两种情况都小于1KB。程序软件必须自己管理这些寄存器（决定要把什么东西放在里面）。

接下来的是缓存，他大多数情况是由硬件来控制的。主存被划分成了许多**缓存行**，一般是64个字节一行，缓存行0的地址从0到63，缓存行1从64到127，如此以往。最频繁访问的缓存行被存放在CPU内部或者非常靠近CPU的高速缓存内。当程序需要从内存中读取一个数据，那么硬件缓存将检查所请求的缓存行是否在缓存中。如果是，那么这就是一次**缓存命中**，请求将直接通过缓存来满足，而不会通过总线向内存发出请求。缓存命中通常耗费两个时钟周期。如果不命中，那么必须去内存中读取，消耗大量时间。因为价格的原因，缓存的容量有限。许多机器拥有两到三级的缓存，每一级都比前一级大一点但同时又慢一点。

缓存行为在很多计算机科学领域都发挥着重要作用，不仅仅是针对RAM的缓存。每当一种资源可以被分割为更小的单位，而其中一部分又比其他部分使用率更高，常常会使用缓存来提升性能。操作系统更是屡试不爽。举例来说，大部分操作系统会将频繁访问的文件（的一部分）缓存在内存中，避免不断地从磁盘上访问他们。同样的，文件全路径到其在磁盘上的位置关系也会被缓存，这样可以减少查找的开销。最后说一点，网页地址（URL）被转换成网络地址（IP）的结果也会被缓存以备将来使用。其他还有很多例子，这里不赘述。

任何缓存系统都会遇到如下一些命题：  
1，什么需要将新的项目加入缓存  
2，新的项目要加到哪个缓存行中  
3，当资源不够时，将哪个项目从缓存中移除  
4，被逐出缓存的项目要放到下一级存储设备的什么地方  

并非所有上述问题都适用于任何缓存情况。对于在CPU缓存中的内存缓存行来说，每次缓存不命中都会将新的项目加入进去。被访问地址的部分高位bits将用来计算具体放入哪一个缓存行中。举例来说，如果是32位地址，有4096个64B的缓存行，那么第6到17bit会用来指定缓存行号，而第0到5bit用来指定在该缓存行内的位置（第几个byte）。这种情况下，要移除的缓存就是要被更新位置的那个，但有些系统不是这样。最后，如果一个缓存行要回写到内存中（如果缓存的内容被修改了），那么回写地址由其对应的缓存地址唯一决定。

缓存是个非常好的主意，现代CPU都有两个缓存。第一级或L1缓存总是在CPU内部，一般负责将解析后的指令送入CPU执行引擎。大多数芯片会有第二块L1缓存，负责缓冲非常频繁访问的数据。L1缓存通常是每块16KB。此外，常常会有一个第二级缓存，称为L2缓存，用于保存几M的最近访问过的数据。L1和L2缓存的区别在于耗时上。访问L1缓存不会有任何时延，而访问L2缓存的时延通常为一到两个时钟周期。

在多核芯片中，设计者不得不对缓存的摆放位置做出抉择。在图1-8（a）中，单独一片L2缓存被多个内核所共享。这种设计被用于因特尔的多核芯片。而图1-8（b）中，每个核都有自己的L2缓存。AMD正是采用了这种设计。每种设计都有其优势和劣势。比如，intel的共享缓存设计需要一个更加复杂的缓存控制器，而AMD的方式则非常难以解决L2缓存一致性。

图1-9所示，内存层级结构的下一级来到了主内存。这是内存系统的主力军。主内存通常称为RAM（随机访问内存）。老江湖有时候会称他们为磁心内存，因为在1950到1960年代，计算机都使用可磁化的铁氧体磁心来只做主内存。虽然已经是几十年前就淘汰了的技术，但是名称保留了下来。现如今，内存都是几十兆到几个G，而且还在迅速增长。所有的不能被缓存命中的CPU请求都会到内存中。

除了主内存，许多计算机都有少量的非易失性随机访问存储介质。不像RAM，非易失性内存不会再系统掉电的时候丢失数据。ROM（只读存储器）在出厂时就已经被编程，而且不能再修改。他们很快，而且不贵。某些计算机中，用于加载引导程序的程序就是放在ROM中。同样的，某些IO设备会使用一些ROM来做底层设备控制。

EEPROM（电可擦除可编程ROM）和闪存都是非易失性的，但是与ROM不同的是他们可以被擦除和重写。然而，对他们的写操作比RAM要慢好几个数量级。所以，他们通常和ROM的用法一样，只是拥有可以现场修改程序bug的附加特性。

闪存也同样经常用于移动电子设备的存储介质。他们在数码相机中被当作胶卷来用，在移动音乐播放器中当作磁盘来用。闪存的速度在内存和磁盘之间。另外，不像磁盘，他们被读写过多次就会损坏。

另一种内存是CMOS，他是易失性的。许多计算机使用CMOS内存保存当前的时间和日期。CMOS内存和用于更新时间的电路都是由一个小电池供电的，所以时间内正常的更新，就算计算机本身已经断电了。CMOS还保存了一些配置信息，比如从哪一块磁盘启动。之所以用CMOS，是因为他们耗电量极低，一般出厂安装的小电池可以维持好几年。但是，一旦电池开始枯竭，计算机会像得了阿尔茨海默综合症那样，开始遗忘记了好多年的东西，比如从哪块盘启动。

###1.3.3 磁盘
下面来说磁盘（硬盘）。磁盘存储价格比RAM低两个数量级，容量高出RAM两个数量级。唯一的问题是随机访问的速度要慢了接近三个数量级。原因是磁盘是机械装置，如图1-10所示。磁盘是有一片或多片金属盘构成的，他们可以以每分钟5400,7200,10800转的速度转动。一个机械臂以一个在磁盘设备一角的轴，在磁盘上方转动，就像在33转每分钟（RPM）的唱片机上的臂。信息是以一系列同心圆的形式写在磁盘上的。给定一个读取臂位置，每个磁头可以读取一个环形区域，称作**轨迹**。在这个给定的读取臂位置上的所有轨迹构成了一个**柱面**。

每条轨迹可以分割为一些扇区，通常一个扇区有512byte。在现代磁盘上，外层的柱面包含了比内部柱面更多的扇区。读取臂从一个柱面移动到另一个柱面需要大约一毫秒。而移动到任意柱面需要5到10毫秒，具体跟磁盘性能有关。一旦读取臂就位，磁盘还要等待需要的扇区转动到磁头下面，这也要花去大约5到10毫秒，取决于磁盘的RPM。一旦扇区也就位，开始读和写，低端磁盘大约是50MB/s，高端磁盘可以达到160MB/s。

有时，人们在讨论的“磁盘”实际上不再是传统意义上的磁盘了，比如SSD（固态磁盘）。SSD没有任何可以移动的组成部件，没有盘状部件，而是将数据保存在闪存中。他与磁盘唯一的共同点就是保存了大量数据且不会在掉电的时候丢失。

许多计算机系统支持**虚拟内存**的设计，我们将以较大篇幅在第三章讨论。这种设计使得运行内存需求大于实际物理内存的程序成为可能，他们可以被放置在磁盘上，然后将物理内存当做缓存，将最频繁访问的部分放在其中。这种设计需要在运行过程中动态进行地址的重映射，将程序生成的地址转化为内存上的物理地址，找到数据真正在RAM中的位置。这种映射工作由CPU内的一个单元完成，称作MMU（内存管理单元），如图1-6所示。

缓存和MMU的出现对性能有很大影响。在一个多进程的系统中，当从一个进程切换到另一个进程时，称作**进程切换**，系统必须要同步所有缓存中的改动，同时改变MMU中存放的地址映射关系寄存器。这两种操作都很费时，程序员花了很大心思去避免他们的发生。后面我们会看到他们的一些策略。

###1.3.4 I/O设备
CPU和内存不是操作系统要管理的全部资源。I/O设备同样和操作系统有着密切联系。正如图1-6所示，I/O设备通常由两部分构成：一个控制器加上设备本身。控制器是单个芯片或一个芯片组，在物理上控制着设备。他从操作系统接收命令，比如，从设备读取数据并将他们传送出去。

大多数情况下，对设备的实际控制是繁琐和复杂的，所以控制器要做的就是向操作系统提供简单的接口（虽然还是很复杂）。举例来说，一个磁盘控制器接到一个命令从第二块磁盘的第11206扇区读取数据。控制器首先需要将这个线性的扇区号换算成对应的柱面扇区和磁头。这种换算过程可能会很复杂，因为外层的柱面的扇区比内层要多，而且有些坏扇区还被重映射到了其他扇区上。然后控制器要根据读取臂的位置来向其发起向内或向外移动的指令，使其移动到请求地址所在的柱面位置。接下来，控制器要等到相应的扇区转动到磁头下，之后才能开始从驱动器上读取bits，去掉信息头，计算校验和。最后，控制器把这些bits组成数据字（可能要考虑字节序）存放到内存中。为了完成这些工作，磁盘控制器通常会内置小的嵌入式计算机，他们来执行实现上述工作的的程序。

另一部分要谈的是设备本身。设备本身的接口非常简单，既是因为他们做不了太多，也是因为需要让他们性成标准。第二条原因是必要的，这样的话，任何磁盘控制器都能适配任何SATA磁盘。SATA是Serial ATA的缩写，ATA是AT Attachment的缩写。如果你还对什么是AT感兴趣，这里解释一下，这是IBM的第二代“个人计算机先进技术”，给予当时非常先进的6MHz的80286处理器，是该公司1984年引入的。从这件事中我们学到的是，计算机产业有个习惯，持续不断的在已存在的缩略词前加上新的前缀和后缀。同样的，我们也学到了，象先进的这一类形容词要非常谨慎的使用，否则30年后会看起来非常可笑。

SATA是现在许多计算机系统的标准硬盘设备。因为真正的设备接口被隐藏在控制器后面，操作系统能看到的只是控制器的接口，这个接口和设备本身的接口是大不相同的。

因为每种类型的控制器都不一样，需要由不同的软件去控制不同的控制器。和控制器打交道的软件被称作设备驱动，主要给控制器发送命令并接受反馈信息。每种控制器的制造商都必须提供每种操作系统支持的设备驱动。比如一种扫描仪需要同时提供支持OS X，windows7，windows8 和Linux的驱动。

设备驱动需要放到操作系统中，这样就能以内核态运行。驱动其实可以在内核之外运行，而且象Linux和windows这样的操作系统现在确实提供了提供了支持用户态驱动的基础设施。绝大部分的驱动程序现在还是跑在内核中。只有很少一部分系统，比如MINIX3，将所有的驱动跑在用户态。在用户态的驱动程序必须被允许以某种受控的方式访问设备，这不是十分的直接。

将驱动放入内核的方法有三种。第一种方法是将驱动和内核重新链接，然后重启系统。许多老一代的UNIX系统采用这种方式。第二种方法是在一个操作系统文件中新建一个条目，告诉操作系统需要加载某个驱动然后重启系统。在启动过程中，操作系统找到需要的驱动并加载。Windows就是这么做的。第三种方法是操作系统可以在运行时动态加载需要的驱动程序而不用重启。这种方法以前很罕见，但是现在却很普遍。可热插拔的设备，如USB和IEEE1394设备（后面会讨论），总是需要动态加载的驱动。

每种控制器都有数量不多的寄存器，用于外界和他通信。比如，一个最简化的磁盘控制器可能会有这样一些寄存器，他们用来指定磁盘地址，内存地址，扇区编号，读写方向。要激活控制器，驱动程序要从控制系统获取命令并将其转化为适当的值，写入这些寄存器中。这些设备寄存器的集合被称作IO port空间，第五章我们将继续这个话题。

在某些操作系统中，设备寄存器还会被映射到操作系统的地址空间中（限于他们可以访问的地址段），于是他们可以被当做内存那样读写。这样的计算机中，没有特殊的IO指令，通过将这些寄存器从用户程序可访问的地址范围中剔除（通过基地址寄存器和限制寄存器？）来防止用户程序直接操作硬件。而其他计算机系统，则将IO设备的寄存器放入一个特殊的IO-port空间（*PIO*），每个寄存器拥有一个端口地址。这些机器提供了特殊的在内核态可用的IN和OUT指令，驱动程序可以通过他们访问设备寄存器。前一种实现避免了使用特殊的指令，但是消耗了一些地址空间资源。后者则反过来，这两种实现都被普遍采用。

输入输出可以通过三种不同的方式实现。最简单的方式是，一个用户程序发起了一个系统调用，然后内核将这个系统调用翻译成了对对应驱动程序的调用。这个驱动程序于是开始IO流程，并且在一个循环中不停的轮询判断这个设备流程是否结束（通常会有某个bit来标记设备是否正忙）。当IO流程结束时，驱动程序把得到的返回数据（如果有）放到该放的地方，然后退出。紧接着，操作系统将控制权返回给用户程序。这种方法被称作忙等。其劣势是，需要完全占用CPU来轮询，直到设备IO流程结束。

第二种方法是，驱动程序启动一个IO流程，并令其在完成时触发一个终端。这之后驱动程序就退出了。操作系统此时会阻塞发起这个IO的程序，以便让其他程序得到执行机会。当控制器发现IO结束时，他会产生一个中断作为完成信号。

中断在操作系统中非常重要，所以我们来详细的检视一下这个方法。在图1-11a中，我们可以看到一个三步的IO流程。第一步，驱动程序通过写控制器的寄存器来告诉他要做什么。然后控制器启动了设备。当控制器完成了被命令读写的数据量后，他通过某个总线通知中断控制器芯片，这是第二步。如果中断控制器刚好可以接收中断（可能由于正在处理更高优先级的中断而不能接收本中断），他会将CPU芯片的某个管脚置位来通知他，这是第三步。第四步，中断控制器将设备的编号写入总线，这样CPU可以读取从而获悉是哪个设备发起了中断（可能同时有很多设备在运行）。

一旦CPU决定执行中断，程序计数器和PSW通常会被压入当前栈，CPU切换到内核态。而设备编号会被当做某一段内存地址的索引，用于查找到对应设备的中断处理程序。这一内存地址称作**中断向量**。一旦中断处理程序（设备驱动程序的一部分）被启动，他将把之前压栈的PC和PSW寄存器清空，然后从设备获取当前状态。当处理程序结束后，他会返回到之前运行的程序的下一条未执行指令。这些步骤在图1-11b中展示。

第三种IO方式利用了一个特殊的硬件：DMA（直接内存访问）芯片，他可以直接控制在设备和内存直接的数据流，而不用CPU的参与。CPU只需设置好DMA芯片，告诉他需要读写的数据量，方向，涉及到的内存和设备的地址，然后就让他自己运行。当DMA完成了任务，就触发一个中断，处理的流程和上面描述的一致。DMA和IO硬件都将在第五章详细描述。

中断可能（常常）在非常不方便的时候发生。比如正在处理其他的中断。为此，CPU有禁止中断并再次使能的功能。当中断被禁止，任何结束任务的设备都仍然发起中断，但是CPU不会被中断，直到中断被再次使能。如果发现之前有多个中断到来，中断控制器会裁决谁先通过，通常是根据静态分配的设备优先级。最高优先级的设备首先得到服务，其他的必须等待。

###1.3.5 总线
图1-6的组织方式被微型计算机和早期的IBM-pc使用了很多年。然而，随着处理器和内存的速度越来越快，单一总线对流量的处理能力已经达到极限。必须要做点什么，于是新的总线加入了，用于连接更高速的IO设备，也用于CPU和memory直接的通信。这种进化的结果，就使得一个大型的x86系统现在看起来像图1-12所示。

这个系统有很多总线（如，cache，内存，PCIe，PCI，USB，SATA和DMI），每个都有不同的传输速率和功能。操作系统必须要了解他们的存在，以便对其配置和管理。主总线是PCIe（Peripheral Component Interconnect Express）总线。

PCIe总线是Intel发明的，用于接替其前任PCI总线，后者又是ISA（Industry Standard Architecture）总线的替代者。PCIe比起前任，快了很多，可以以几十G每秒的速率传输。他的特性也非常不同。直到他诞生的2004年之前，总线大多是并行的，且是共享的。共享的意思是许多设备使用同一条总线传输数据。所以当多个设备同时发送数据时，必须要有个仲裁器去决定谁能使用总线。与之不同的是，PCIe使用了专用的端到端的连接。传统PCI总线使用的并行总线结构意味着每个数据字都是通过并行的多条导线传输的。例如，在常规的PCI总线上传输一个32-bit的数据要通过32根并行的导线。PCIe则使用了串行总线结构，将所有的比特放在一个消息中传输，通过一个单线连接，称作一路，非常类似于网络的数据包。这样的实现要简单的多，因为你不用保证所有32bit精确地同时到达目的地。并行的理念还是会使用，比如你可以将多个路并行起来。比如，我们可以用32路并行的发送32个消息。随着外设的速度日新月异，比如网卡和显卡，PCIe标准每隔3-5年都要升级一次。比如，16路的PCIe2.0可以提供64Gb每秒的速率，3.0的速率是其两倍，4.0又会翻一番。

同时，我们还有很多支持老PCI标准的旧设备。如图1-12所示，这些设备连接在一个单独的hub芯片上。未来当我们不仅将PCI视作*老*，而是看成古董的时候，很可能会将他们连接在另外一个hub上，然后再连接在主hub上，构成了总线树。

在这种配置中，CPU和内存通过快速DDR3总线通信，和外部显卡通过PCIe通信，而其他所有设备连接在一个hub上再通过DMI（Direct Media  Interface）总线和CPU通信。hub通过通用串行总线和USB设备相连，通过SATA总线和硬盘以及DVD设备通信，通过PCIe总线和以太网设备联系。老的PCI设备则通过PCI总线与hub相连。

另外，每个CPU的核都有自己的cache加上一个大家共享的大的cache，每种cache都有自己的总线。

USB（(Universal Serial Bus）的发明是为了将一些慢速设备连接到计算机上，比如键盘和鼠标。然而，把一个以5Gbps速度嗖嗖的传输数据的现代USB3.0设备称作慢速设备，对那些8Mbps ISA当做主总线的IBM PC时代成长起来的人来说十分不自然。USB使用一个小的有4根或11根线的接头（取决于版本），其中一些导线是为USB设备供电的，或者提供接地。USB是一种集中式的总线，有一个根设备会每隔1ms扫描一遍总线来轮询是否有设备发生传输行为。USB1.0可以处理总计12Mbps的流量，USB2.0上升到480Mbps，USB3.0最高可达到不低于5Gbps的速度。任何USB设备都可以连上电脑就立即可以投入使用，不需要重启，而一些USB之前的设备却必须重启后才能使用，这一特性惊呆了之前沮丧的用户们。

SCSI（Small Computer System Interface）总线是一种高性能总线，专为高速磁盘，扫描仪，和其他对带宽要求非常高的设备而生。如今，他们大都在服务器和工作站中使用。他们可以达到640MB/s的速度。

要在图1-12的环境下工作，操作系统必须清楚哪些设备被连接到计算机上，并配置他们。这种需求迫使Intel和微软开发了即插即用设备，基于最先实现在苹果的mac系统上的相似概念。在即插即用设备之前，每个IO设备都有固定的中断号和固定的IO地址空间。举例来说，键盘使用中断号1，IO地址为0x60到0x64，软皮控制器为中断6，IO地址为0x3f0~0x3f7，打印机为中断7,0x378~0x37A，如此等等。

到此相安无事，直到某一天，一个用户给计算机装上了一个调制解调器卡和一块声卡，他们恰巧使用了同一个中断号4。于是冲突产生了，他们无法同时工作。解决方案是在办卡上加入DIP开关或跳线，让用户自己配置中断号和IO地址，自行避免使用冲突的资源。年轻的极客们或许没有问题，但是对其他人来说只会造成混乱。

即插即用设备做的就是让系统自动收集IO设备的信息，集中地分配中断和IO地址资源，然后为每个设备配置好。这些工作和计算机启动过程密切结合，所以让我们来研究一下，这可不是简单的小事。

###1.3.6 启动计算机
简单的说，计算机启动过程如下。每个PC都有个父母板（在计算机工业遭遇政治正确之前称作母板）。父母板上有个程序称之为系统BIOS（Basic Input Output System）。BIOS包含了底层IO软件，包括读取键盘输入，打印到屏幕，disk IO的程序等。现如今，这段程序保存在flash RAM中，他是非易失性的，同时也可以在BIOS发现软件bug时被操作系统更新。

当系统启动时，BIOS首先启动。他首先检查系统有多少RAM，键盘和其他基础设备是否安装并工作正常。他通过扫描PCIe和PCI总线来探测所有安装的设备。如果发现有不同于上次启动时的设备，就配置那些新设备。

紧接着，BIOS尝试从CMOS存储的一个设备列表中找到启动设备。用户可以改变这个列表，方法是在系统刚刚开始启动的时候进入BIOS设置。典型的情况是，先从CD-ROM（或者USB）驱动器启动，如果存在的话。如果失败了，就从硬盘启动。启动设备的第一个扇区被读入内存执行。这个扇区存放着检测硬盘分区表的程序，分区表就紧邻在第一个扇区后面的位置，检测程序会找到哪个分区是激活了的。然后二级启动加载程序从那个分区读出并执行。这个加载程序将从激活的分区读取操作系统并启动他。

操作系统此时再回过头去向BIOS查询获取系统配置信息。对每个设备，他将检查自己是否有其驱动程序。如果没有，那么他会要求用户插入一个驱动程序光盘（由设备生产商提供）或者从网络上下载驱动程序。一旦他拥有了所有的驱动程序，他将把这些驱动加载到内核。然后他会初始化一些表（IDT等），创建好任何需要的后台进程，启动一个登陆程序或者GUI。
